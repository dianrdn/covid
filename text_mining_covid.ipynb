{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_mining_covid.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dianrdn/covid/blob/master/text_mining_covid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awnEsM81kACc",
        "colab_type": "text"
      },
      "source": [
        "# **Coronavirus Vaccine**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6g9GloB_kz_d",
        "colab_type": "text"
      },
      "source": [
        "Coronavirus disease 2019 (COVID-19) is an infectious disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). It was first identified in December 2019 in Wuhan, Hubei, China, and has resulted in an ongoing pandemic. The first confirmed case has been traced back to 17 November 2019 in Hubei.As of 6 August 2020, more than 18.7 million cases have been reported across 188 countries and territories, resulting in more than 706,000 deaths. More than 11.3 million people have recovered.\n",
        "\n",
        "Until now, a vaccine for this disease has not been found. However, lately a lot of parties have started to raise rumors about the issue of vaccine findings.\n",
        "\n",
        "Most of the people today share opinions and information about COVID-19 vaccine through various social media, including Twitter. Today we will analyze the content of conversations between Twitter users regarding the keyword \"vaccine\" and \"vaccines\" in English."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTFmncw78173",
        "colab_type": "text"
      },
      "source": [
        "We use text mining technique to analyze this textual data. Text mining is a process of exploring sizeable textual data and find patterns. Text Mining process the text itself. Finding frequency counts of words, length of the sentence, presence/absence of specific words is known as text mining.\n",
        "\n",
        "Natural language processing is one of the components of text mining. NLP helps identified sentiment, finding entities in the sentence, and category of blog/article. Text mining is preprocessed data for text analytics. In Text Analytics, statistical and machine learning algorithm used to classify information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2afMRC54AJkM",
        "colab_type": "text"
      },
      "source": [
        "## **A. Text Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i14aOA1Tm0x0",
        "colab_type": "text"
      },
      "source": [
        "Text preprocessing is traditionally an important step for natural language processing (NLP) tasks. It transforms text into a more digestible form so that machine learning algorithms can perform better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4gYhrbtC4rl",
        "colab_type": "text"
      },
      "source": [
        "***Import Library***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umR5WapdlW_C",
        "colab_type": "text"
      },
      "source": [
        "We need to import some libraries first. Here are the libraries we need to import."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4rKM6x-C97u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import library for Text Analytics\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhqJUzVl_DDq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Libraries for Data Manipulation\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KI9l6zjVDFVt",
        "colab_type": "text"
      },
      "source": [
        "***Import Data***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64Md_yy4lwh-",
        "colab_type": "text"
      },
      "source": [
        "Then, import our demonetization tweets dataset into this notebook using Pandas library. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duG0Aw--96b6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Data\n",
        "tweets=pd.read_csv('https://raw.githubusercontent.com/sma-health/data/master/covid_dataset.csv')\n",
        "tweets.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfbBE4wVm2vC",
        "colab_type": "text"
      },
      "source": [
        "***Select Data***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMYa8Kw4t4kf",
        "colab_type": "text"
      },
      "source": [
        "We will use only text(tweets) data in this text mining modeling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUS3DkJj_dsI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select Only Text Column\n",
        "text = tweets[['text']]\n",
        "text.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mydcGr66m8tp",
        "colab_type": "text"
      },
      "source": [
        "***Clean the Dataset***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRlihIkS_5I7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create Text Cleaning Function using Regex\n",
        "import re\n",
        "\n",
        "def  clean_text(df, text_field):\n",
        "    df[text_field] = df[text_field].str.lower()\n",
        "    df[text_field] = df[text_field].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))  \n",
        "    # remove numbers\n",
        "    df[text_field] = df[text_field].apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n",
        "    return df\n",
        "\n",
        "# Apply to the data\n",
        "text_clean = clean_text(text, 'text')\n",
        "text_clean.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gclqDLHICQ0G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Stopword\n",
        "import nltk.corpus\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Apply Stopword to the dataframe\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "text_clean['nostopword'] = text_clean['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "text_clean.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xar0blD5mXsN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Punkt\n",
        "import nltk \n",
        "nltk.download('punkt')\n",
        "\n",
        "# Tokenize\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "text_clean['tokenize'] = text_clean['nostopword'].apply(lambda x: word_tokenize(x))\n",
        "text_clean.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBF-5Jxsm77r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "outputId": "02495d84-6e3e-4804-8180-e914ceeade00"
      },
      "source": [
        "# Import Stemmer\n",
        "from nltk.stem import PorterStemmer \n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Create Stemmer Function\n",
        "def word_stemmer(text):\n",
        "    stem_text = [PorterStemmer().stem(i) for i in text]\n",
        "    return stem_text\n",
        "\n",
        "# Apply to the dataframe\n",
        "text_clean['stemming'] = text_clean['tokenize'].apply(lambda x: word_stemmer(x))\n",
        "text_clean.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>nostopword</th>\n",
              "      <th>tokenize</th>\n",
              "      <th>stemming</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tens o covid</td>\n",
              "      <td>tens covid</td>\n",
              "      <td>[tens, covid]</td>\n",
              "      <td>[ten, covid]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>the white house ordered  change in guidelines ...</td>\n",
              "      <td>white house ordered change guidelines longer r...</td>\n",
              "      <td>[white, house, ordered, change, guidelines, lo...</td>\n",
              "      <td>[white, hous, order, chang, guidelin, longer, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>you might have a bigger death toll than covid</td>\n",
              "      <td>might bigger death toll covid</td>\n",
              "      <td>[might, bigger, death, toll, covid]</td>\n",
              "      <td>[might, bigger, death, toll, covid]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>the covid pandemic has made humanity experienc...</td>\n",
              "      <td>covid pandemic made humanity experience unprec...</td>\n",
              "      <td>[covid, pandemic, made, humanity, experience, ...</td>\n",
              "      <td>[covid, pandem, made, human, experi, unpreced,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>corona is ruining your all plans for  save the...</td>\n",
              "      <td>corona ruining plans save also</td>\n",
              "      <td>[corona, ruining, plans, save, also]</td>\n",
              "      <td>[corona, ruin, plan, save, also]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  ...                                           stemming\n",
              "0                                       tens o covid  ...                                       [ten, covid]\n",
              "1  the white house ordered  change in guidelines ...  ...  [white, hous, order, chang, guidelin, longer, ...\n",
              "2      you might have a bigger death toll than covid  ...                [might, bigger, death, toll, covid]\n",
              "3  the covid pandemic has made humanity experienc...  ...  [covid, pandem, made, human, experi, unpreced,...\n",
              "4  corona is ruining your all plans for  save the...  ...                   [corona, ruin, plan, save, also]\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YnfZR9EnnsP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "outputId": "5b1b6c30-66c0-45f7-cd0d-0ffb6015bb2a"
      },
      "source": [
        "# Import Wordnet Library\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Create Lematization Funtion\n",
        "def word_lemmatizer(text):\n",
        "    lem_text = [WordNetLemmatizer().lemmatize(i, pos='v') for i in text]\n",
        "    return lem_text\n",
        "\n",
        "# Apply to a dataframe\n",
        "text_clean['lemmatization'] = text_clean['tokenize'].apply(lambda x: word_lemmatizer(x))\n",
        "text_clean.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>nostopword</th>\n",
              "      <th>tokenize</th>\n",
              "      <th>stemming</th>\n",
              "      <th>lemmatization</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tens o covid</td>\n",
              "      <td>tens covid</td>\n",
              "      <td>[tens, covid]</td>\n",
              "      <td>[ten, covid]</td>\n",
              "      <td>[tens, covid]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>the white house ordered  change in guidelines ...</td>\n",
              "      <td>white house ordered change guidelines longer r...</td>\n",
              "      <td>[white, house, ordered, change, guidelines, lo...</td>\n",
              "      <td>[white, hous, order, chang, guidelin, longer, ...</td>\n",
              "      <td>[white, house, order, change, guidelines, long...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>you might have a bigger death toll than covid</td>\n",
              "      <td>might bigger death toll covid</td>\n",
              "      <td>[might, bigger, death, toll, covid]</td>\n",
              "      <td>[might, bigger, death, toll, covid]</td>\n",
              "      <td>[might, bigger, death, toll, covid]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>the covid pandemic has made humanity experienc...</td>\n",
              "      <td>covid pandemic made humanity experience unprec...</td>\n",
              "      <td>[covid, pandemic, made, humanity, experience, ...</td>\n",
              "      <td>[covid, pandem, made, human, experi, unpreced,...</td>\n",
              "      <td>[covid, pandemic, make, humanity, experience, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>corona is ruining your all plans for  save the...</td>\n",
              "      <td>corona ruining plans save also</td>\n",
              "      <td>[corona, ruining, plans, save, also]</td>\n",
              "      <td>[corona, ruin, plan, save, also]</td>\n",
              "      <td>[corona, ruin, plan, save, also]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  ...                                      lemmatization\n",
              "0                                       tens o covid  ...                                      [tens, covid]\n",
              "1  the white house ordered  change in guidelines ...  ...  [white, house, order, change, guidelines, long...\n",
              "2      you might have a bigger death toll than covid  ...                [might, bigger, death, toll, covid]\n",
              "3  the covid pandemic has made humanity experienc...  ...  [covid, pandemic, make, humanity, experience, ...\n",
              "4  corona is ruining your all plans for  save the...  ...                   [corona, ruin, plan, save, also]\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-ka-0qPowMu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "caf5449e-9797-43cb-d988-e8615e402ae5"
      },
      "source": [
        "# Convert to a New Dataframe\n",
        "text_preprocessed = text_clean['lemmatization'].str.join(\",\") \n",
        "text_preprocessed = text_preprocessed.str.replace(',', ' ', regex=False)\n",
        "text_preprocessed = pd.DataFrame(text_preprocessed)\n",
        "text_preprocessed.rename(columns={'lemmatization': 'text'}, inplace = True)\n",
        "text_preprocessed"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tens covid</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>white house order change guidelines longer rec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>might bigger death toll covid</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>covid pandemic make humanity experience unprec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>corona ruin plan save also</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17485</th>\n",
              "      <td>miliband firm downsize outsource years covid k...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17486</th>\n",
              "      <td>hear st night accept nomination finally nation...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17487</th>\n",
              "      <td>houseplants ease covid stress boost mental hea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17488</th>\n",
              "      <td>best love spell miami best miami love spell ca...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17489</th>\n",
              "      <td>every day case covid increase yet government p...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>17490 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text\n",
              "0                                             tens covid\n",
              "1      white house order change guidelines longer rec...\n",
              "2                          might bigger death toll covid\n",
              "3      covid pandemic make humanity experience unprec...\n",
              "4                             corona ruin plan save also\n",
              "...                                                  ...\n",
              "17485  miliband firm downsize outsource years covid k...\n",
              "17486  hear st night accept nomination finally nation...\n",
              "17487  houseplants ease covid stress boost mental hea...\n",
              "17488  best love spell miami best miami love spell ca...\n",
              "17489  every day case covid increase yet government p...\n",
              "\n",
              "[17490 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNDdgELqtgt7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save as CSV\n",
        "text_preprocessed.to_csv('text_preprocessed_covid.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_K6wk7YAap_",
        "colab_type": "text"
      },
      "source": [
        "## **B. Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuSavPiKkSQV",
        "colab_type": "text"
      },
      "source": [
        "Text classification is the process of assigning tags or categories to text according to its content. It’s one of the fundamental tasks in Natural Language Processing (NLP) with broad applications such as sentiment analysis, topic labeling, spam detection, and intent detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Rrtw8ARM7H4b"
      },
      "source": [
        "***Import Library***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRusYti47L66",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import library for Text Analytics\n",
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rH-MTmU7UDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Libraries for Data Manipulation\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFtFP_hB7XJ_",
        "colab_type": "text"
      },
      "source": [
        "***Modeling Sentiment Analysis***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4D8U5jI-w1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Module\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
        "\n",
        "# Sentiment Analysis\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "listy = [] \n",
        "for index, row in text_preprocessed.iterrows():\n",
        "  ss = sid.polarity_scores(row['text'])\n",
        "  listy.append(ss)\n",
        "  \n",
        "se = pd.Series(listy)\n",
        "text_preprocessed['polarity'] = se.values\n",
        "display(text_preprocessed.head(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stJP2xoR_PM6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pie Chart Visualization\n",
        "labels = ['negative', 'neutral', 'positive']\n",
        "sizes  = [ss['neg'], ss['neu'], ss['pos']]\n",
        "plt.pie(sizes, labels=labels, autopct='%1.1f%%')\n",
        "plt.axis('equal') \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zItUWz6kyTgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save as CSV\n",
        "text_preprocessed.to_csv('sentiment_covid.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDiwU2gV_eRY",
        "colab_type": "text"
      },
      "source": [
        "## **C. Topic Modeling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YC8fK2kzfW4j",
        "colab_type": "text"
      },
      "source": [
        "Topic model is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: \"dog\" and \"bone\" will appear more often in documents about dogs, \"cat\" and \"meow\" will appear in documents about cats, and \"the\" and \"is\" will appear equally in both. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FG2t_7n6fawH",
        "colab_type": "text"
      },
      "source": [
        "***Install Library, Import Libraries, and Import Modules***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxM8QfGWRt0r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        },
        "outputId": "6966dd18-7c3c-4bea-f9df-88a8d467d90f"
      },
      "source": [
        "# Install Library\n",
        "! pip install pyLDAvis"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyLDAvis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/3a/af82e070a8a96e13217c8f362f9a73e82d61ac8fff3a2561946a97f96266/pyLDAvis-2.1.2.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.35.1)\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.0.5)\n",
            "Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.11.2)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.7.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (3.6.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Collecting funcy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/4b/6ffa76544e46614123de31574ad95758c421aae391a1764921b8a81e1eae/funcy-1.14.tar.gz (548kB)\n",
            "\u001b[K     |████████████████████████████████| 552kB 49.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2018.9)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.15.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (20.1.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.9.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (49.6.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.4.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (8.4.0)\n",
            "Building wheels for collected packages: pyLDAvis, funcy\n",
            "  Building wheel for pyLDAvis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-2.1.2-py2.py3-none-any.whl size=97712 sha256=5086323545bb05299d591a153e018d894c108afaec1ba4f9395fcea6766b683a\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/71/24/513a99e58bb6b8465bae4d2d5e9dba8f0bef8179e3051ac414\n",
            "  Building wheel for funcy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for funcy: filename=funcy-1.14-py2.py3-none-any.whl size=32042 sha256=f34d0ea666b052138c7c476c6deafa39acdd89cf1db5bce541ab7953a18386bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/20/5a/d8/1d875df03deae6f178dfdf70238cca33f948ef8a6f5209f2eb\n",
            "Successfully built pyLDAvis funcy\n",
            "Installing collected packages: funcy, pyLDAvis\n",
            "Successfully installed funcy-1.14 pyLDAvis-2.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LTY4ilBSCdI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Libraries\n",
        "import nltk\n",
        "import os\n",
        "import numpy as np, pyLDAvis, pyLDAvis.sklearn; pyLDAvis.enable_notebook()\n",
        "\n",
        "# Import Modules\n",
        "from __future__ import print_function \n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSkNONYpSEXl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "07225898-0cb8-4997-8211-5a72bc2f8931"
      },
      "source": [
        "# Clone Library and Data from Github\n",
        "! git clone https://github.com/sma-health/tm\n",
        "\n",
        "# Set Data Directory\n",
        "os.chdir('tm')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'tm'...\n",
            "remote: Enumerating objects: 20, done.\u001b[K\n",
            "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 20 (delta 4), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (20/20), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVEv1nVgG9K5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "1a8ca805-7bcf-4bd5-a3de-f4aaf3bb23bf"
      },
      "source": [
        "# Import Stop Words\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Import Data\n",
        "text_preprocessed = 'text_preprocessed_covid19.csv'\n",
        "\n",
        "# Load Tweets Data\n",
        "import MyLib as TS\n",
        "Tweets = TS.LoadTxt(text_preprocessed) \n",
        "print('Total loaded tweets = {0}'.format(len(Tweets)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 17491/17491 [00:00<00:00, 1913323.72it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Total loaded tweets = 17491\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdDudz3lgWy4",
        "colab_type": "text"
      },
      "source": [
        "***Set Number of Topics, Top Topics, Top Words***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VK4J4At2gRwo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_topics = 4\n",
        "top_topics = 4\n",
        "top_words = 8"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AL5bu8Tld7S",
        "colab_type": "text"
      },
      "source": [
        "***Feature Extraction***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_w25SADrrPf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Feature Extraction\n",
        "count_vector = CountVectorizer(lowercase = True, token_pattern = r'\\b[a-zA-Z]{3,}\\b') \n",
        "dtm_tf = count_vector.fit_transform(Tweets)\n",
        "tf_terms = count_vector.get_feature_names()\n",
        "del Tweets"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aS5jGV0up2p",
        "colab_type": "text"
      },
      "source": [
        "***Show Topic***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcMlhTzUuyrk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "outputId": "bccbcd77-e4a3-4753-9b62-4158dc54f1d4"
      },
      "source": [
        "# Topic Search Function\n",
        "lda_tf = LatentDirichletAllocation(n_components=n_topics, learning_method='online', random_state=0).fit(dtm_tf)\n",
        "\n",
        "# Show Topics\n",
        "vsm_topics = lda_tf.transform(dtm_tf); doc_topic =  [a.argmax()+1 for a in tqdm(vsm_topics)] # topic of docs\n",
        "print('In total there are {0} major topics, distributed as follows'.format(len(set(doc_topic))))\n",
        "plt.hist(np.array(doc_topic), alpha=0.5); plt.show()\n",
        "print('Printing top {0} Topics, with top {1} Words:'.format(top_topics, top_words))\n",
        "TS.print_Topics(lda_tf, tf_terms, top_topics, top_words)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 17491/17491 [00:00<00:00, 699690.71it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "In total there are 4 major topics, distributed as follows\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Printing top 4 Topics, with top 8 Words:\n",
            "Topic #0:\n",
            "covid school corona work get home see good\n",
            "Topic #1:\n",
            "covid amp trump every people bring right die\n",
            "Topic #2:\n",
            "covid corona get students dont help think want\n",
            "Topic #3:\n",
            "covid case new test coronavirus mask health positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRG28QyXYPwL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 861
        },
        "outputId": "c7ea376f-283a-44a0-af00-ff393ff618f5"
      },
      "source": [
        "# Interactively visualizing the Topics, please ignore the Warnings\n",
        "# Wait few minutes and then hover the Mouse over the Topics to Explore\n",
        "pyLDAvis.sklearn.prepare(lda_tf, dtm_tf, count_vector) "
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el1081401495244848884715974756\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el1081401495244848884715974756_data = {\"mdsDat\": {\"x\": [-0.11473086362968671, -0.04058058438834943, 0.2721372302539457, -0.11682578223590971], \"y\": [-0.06413600474679038, -0.18737599652034256, 0.03707519525812538, 0.21443680600900752], \"topics\": [1, 2, 3, 4], \"cluster\": [1, 1, 1, 1], \"Freq\": [31.614771253910547, 26.75889758301099, 24.134130069411004, 17.492201093667443]}, \"tinfo\": {\"Term\": [\"case\", \"trump\", \"new\", \"corona\", \"coronavirus\", \"every\", \"mask\", \"students\", \"health\", \"school\", \"amp\", \"bring\", \"test\", \"work\", \"help\", \"think\", \"get\", \"right\", \"want\", \"home\", \"see\", \"good\", \"still\", \"come\", \"everyone\", \"leave\", \"never\", \"risk\", \"die\", \"would\", \"students\", \"help\", \"think\", \"want\", \"exam\", \"exams\", \"postpone\", \"tell\", \"sir\", \"may\", \"neet\", \"jee\", \"pay\", \"happen\", \"call\", \"govt\", \"mean\", \"didnt\", \"situation\", \"next\", \"open\", \"doesnt\", \"return\", \"thing\", \"watch\", \"student\", \"free\", \"wont\", \"infect\", \"control\", \"conduct\", \"please\", \"give\", \"corona\", \"due\", \"dont\", \"get\", \"need\", \"covid\", \"time\", \"one\", \"like\", \"know\", \"take\", \"say\", \"people\", \"also\", \"year\", \"even\", \"amp\", \"india\", \"test\", \"trump\", \"every\", \"bring\", \"leave\", \"everyone\", \"never\", \"still\", \"best\", \"education\", \"worst\", \"child\", \"decision\", \"plan\", \"children\", \"americans\", \"corner\", \"nep\", \"years\", \"follow\", \"post\", \"flu\", \"learn\", \"lie\", \"behindnepconbykiit\", \"market\", \"become\", \"believe\", \"biden\", \"rise\", \"high\", \"right\", \"affect\", \"come\", \"would\", \"amp\", \"die\", \"covid\", \"people\", \"work\", \"world\", \"live\", \"like\", \"deaths\", \"pandemic\", \"say\", \"many\", \"mask\", \"case\", \"coronavirus\", \"risk\", \"update\", \"today\", \"social\", \"read\", \"business\", \"increase\", \"distance\", \"talk\", \"million\", \"without\", \"travel\", \"issue\", \"total\", \"confirm\", \"countries\", \"break\", \"record\", \"research\", \"recover\", \"measure\", \"provide\", \"play\", \"information\", \"announce\", \"outbreak\", \"train\", \"health\", \"support\", \"august\", \"new\", \"report\", \"test\", \"positive\", \"covid\", \"news\", \"via\", \"number\", \"wear\", \"deaths\", \"vaccine\", \"pandemic\", \"see\", \"find\", \"school\", \"good\", \"first\", \"safe\", \"house\", \"kill\", \"hope\", \"great\", \"things\", \"check\", \"kid\", \"love\", \"white\", \"consider\", \"ive\", \"blame\", \"fuck\", \"nurse\", \"book\", \"age\", \"level\", \"set\", \"especially\", \"wonder\", \"require\", \"flight\", \"prove\", \"word\", \"europe\", \"tweet\", \"thank\", \"others\", \"really\", \"close\", \"home\", \"see\", \"work\", \"fight\", \"covid\", \"make\", \"care\", \"corona\", \"get\", \"people\", \"know\", \"back\", \"take\", \"virus\", \"way\"], \"Freq\": [1244.0, 988.0, 979.0, 1584.0, 654.0, 685.0, 611.0, 724.0, 604.0, 470.0, 1639.0, 584.0, 1297.0, 804.0, 607.0, 605.0, 1578.0, 538.0, 577.0, 544.0, 614.0, 335.0, 440.0, 481.0, 419.0, 418.0, 416.0, 383.0, 680.0, 561.0, 723.8102929710776, 606.3607912514972, 604.7923769031324, 576.3589712731098, 460.4439271646532, 458.03213543107177, 360.55541205293554, 319.732481014312, 301.8510645895109, 310.4895335535869, 282.8123943871878, 276.7310625200633, 267.58104477891015, 267.7075713387742, 273.01637168749374, 246.58126624845056, 247.96939968413145, 230.1070233190438, 223.57685594941265, 212.35557452057927, 215.92430094942625, 208.0193144941041, 221.65788541055394, 190.11261546890768, 189.6307561109398, 172.1724351314461, 187.35114054720123, 165.8388532971583, 149.28322129718748, 152.45339091791843, 241.5133405440935, 491.60860144063537, 456.296739844466, 1195.316292515164, 575.1442913889724, 628.1161340642972, 1028.8852477733135, 464.35890723148356, 3518.332110194888, 515.3371758447479, 469.7778256912921, 464.30283253033764, 442.6042563827641, 429.99703520924703, 448.6970490591077, 516.9027726460275, 316.7387353763066, 304.485155423861, 316.8763091102271, 397.4234965092997, 295.0193731565471, 303.2802861913356, 987.5433445622596, 684.318447229151, 583.9423077551344, 418.03996252872355, 418.57881653674065, 416.12194322911324, 439.49295443167773, 387.1211197601591, 320.3013970025041, 280.7328947244535, 277.4518539481912, 272.4179936266576, 281.5688254553407, 257.92497185099785, 246.76983701510622, 215.0503980874025, 207.6753078279239, 213.73440821568647, 235.84817497038526, 197.9039971548609, 190.18637631463875, 192.7041591750999, 187.60527938826613, 174.34485260077807, 172.57816073168533, 164.85291345726472, 154.8247679903547, 140.70319348068745, 187.99693552682018, 148.42979729372888, 534.3885202490834, 209.90071325128713, 469.6016632760773, 509.1550691256067, 1137.090802145333, 528.6989698540917, 3909.624630366428, 658.2010151505378, 416.2280321977987, 264.87510930183925, 296.9174239184504, 311.9495386897413, 277.07595184846457, 290.3661346290511, 286.4362725246072, 244.8096816079465, 610.8633807320055, 1242.3064956120559, 653.1630934491544, 382.8429539560305, 277.431222694161, 290.34927481371653, 286.5122194637691, 243.62553111651087, 226.23629528217216, 219.16116213116936, 230.85279385268402, 235.42496154844338, 204.4308984691506, 208.29439275689901, 200.86110127396594, 234.44641171633396, 170.23629066534428, 167.6150449948052, 163.51560227138668, 165.8213479367194, 151.5611927843593, 147.26345266452836, 135.90180147045447, 141.4766105838992, 137.43725587002064, 149.30412486119016, 127.48514092605888, 126.30029730240646, 124.14820667291671, 122.11229089351835, 597.3758360107929, 347.5814606247448, 184.39264616486366, 931.9965611730652, 349.0150084512547, 878.2924373822208, 403.4773092959285, 4183.07063053258, 309.10063351132175, 308.86169441031166, 267.3433090390993, 214.1766143695286, 273.5878144218707, 242.0746964804312, 280.85595983680173, 245.6992227025847, 219.16456832479122, 470.2077054234529, 334.4356360224075, 284.8932966062012, 264.15216001824956, 246.053881587466, 229.5306158602272, 192.72999336052314, 190.48807434156538, 186.65096259182272, 181.47168910584952, 176.23191819049387, 163.979579003483, 159.41854115507454, 160.09059990455094, 150.2061688677613, 145.2293789909345, 135.29378091522935, 129.52947289914738, 132.14337975054457, 109.17902569428367, 114.3499897593559, 106.05486109880233, 78.43555089970978, 80.16992151610337, 80.2646603621539, 73.61169170298491, 78.1157139996626, 76.83090346241826, 75.53059697565216, 77.52122840942884, 265.9188881769069, 114.62403415067398, 254.22829567333417, 174.59853668002842, 372.6403402045714, 365.73925307365573, 387.49900748403024, 157.54547182669762, 1757.0529878264583, 319.2588448504101, 216.3663074434336, 388.4433237034956, 374.32070797595117, 265.23568503655093, 208.7618778849497, 181.2466113764457, 182.2096149200904, 162.94836677859237, 159.88275385941714], \"Total\": [1244.0, 988.0, 979.0, 1584.0, 654.0, 685.0, 611.0, 724.0, 604.0, 470.0, 1639.0, 584.0, 1297.0, 804.0, 607.0, 605.0, 1578.0, 538.0, 577.0, 544.0, 614.0, 335.0, 440.0, 481.0, 419.0, 418.0, 416.0, 383.0, 680.0, 561.0, 724.4939093428708, 607.0938452444708, 605.5320754908854, 577.0866098469623, 461.12080112650705, 458.71317501746574, 361.233921422463, 320.45195514212395, 302.53186779762575, 311.2216439497369, 283.4884633693813, 277.40700363041964, 268.2875068445419, 268.42068424518004, 273.7648108609991, 247.27902751942304, 248.69285037265712, 230.82296624278555, 224.30298632929257, 213.07361290689892, 216.6566395776256, 208.7592428191782, 222.4836108858536, 190.8411244792715, 190.36741893811265, 172.85789137137556, 188.11621457209327, 166.5449254574353, 150.00326214639793, 153.19537116278522, 243.497550931458, 503.12476249976316, 509.87842197392524, 1584.3177721357483, 700.7956666769011, 787.7345797317312, 1578.6702024404374, 656.6786489894761, 13368.080358920355, 807.5780215943108, 738.051737396932, 777.6631985135464, 735.1791662652646, 769.2320132847877, 938.1089473135335, 1498.0510162117082, 486.798592249781, 430.8963137329086, 523.2090838060873, 1639.279611966885, 470.5705694179866, 1297.9794941708662, 988.4075165789302, 685.0050979467978, 584.6272528476633, 418.73095596853693, 419.27144900204365, 416.81316558684506, 440.2363948475783, 387.81694865784084, 320.9889934742388, 281.41188910293033, 278.13636757504116, 273.11212732264624, 282.29445815756816, 258.6606532759436, 247.50020563714557, 215.72119004239698, 208.3444862426976, 214.45201493215626, 236.70672083561362, 198.6343393280995, 190.89395105750455, 193.43049310020376, 188.31286744316472, 175.01394823258062, 173.2962018253171, 165.5754723451926, 155.53288249898287, 141.40538182072962, 188.94234376252044, 149.1764796520959, 538.2684881678795, 210.9974542859328, 481.635048592528, 561.8948067689686, 1639.279611966885, 680.0749937367998, 13368.080358920355, 1498.0510162117082, 804.2194775463904, 387.22427702383624, 506.38663491846853, 777.6631985135464, 562.7602600357176, 859.9431774216157, 938.1089473135335, 557.6799916257504, 611.7482151150405, 1244.1417176112257, 654.1687969270087, 383.7559906394934, 278.1218015941943, 291.0811961271215, 287.24395722974515, 244.34982330108764, 226.9575420220401, 219.88715479197222, 231.65298097257164, 236.26084778746107, 205.1586228714326, 209.06443906357416, 201.61085906903818, 235.34881468778664, 170.93197336337084, 168.30368691670762, 164.22635569046327, 166.5643458357737, 152.27464945603566, 148.01744869979711, 136.60233248587818, 142.208295531534, 138.15751065763615, 150.0943798045281, 128.1904162794216, 127.01813059864419, 124.87385598876274, 122.84264264656838, 604.0377866994373, 353.6462187808592, 186.2238647852284, 979.801428126958, 378.6418506994981, 1297.9794941708662, 536.883039873243, 13368.080358920355, 400.14529865580414, 448.06732323028433, 378.09640889898225, 261.56169174920444, 562.7602600357176, 441.77482656198185, 859.9431774216157, 614.3837928570447, 370.79932313813305, 470.93395909547627, 335.16368817633156, 285.64592235122836, 264.87112478153455, 246.76362182793696, 230.25488873421034, 193.45546120879933, 191.22095285439883, 187.38018911933617, 182.20597206323606, 176.94627589250666, 164.6882536623195, 160.12218631097602, 160.83131298831196, 150.93692995751036, 145.95498711945527, 136.00532664848635, 130.23074500992942, 132.87915374882672, 109.91088112536016, 115.13503623879265, 106.7972586276225, 79.15093662615378, 80.90969122304999, 81.0207618831858, 74.31723625672994, 78.86720059990483, 77.58713405811108, 76.28846044036564, 78.31583900714091, 273.5651522247386, 116.44586065177933, 284.8415417191038, 201.4372228335566, 544.7254842952947, 614.3837928570447, 804.2194775463904, 231.64726711936217, 13368.080358920355, 809.1338661712361, 414.71494325052646, 1584.3177721357483, 1578.6702024404374, 1498.0510162117082, 735.1791662652646, 539.0511850721091, 769.2320132847877, 557.8112749305457, 345.7276281611965], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.7447, -4.9217, -4.9243, -4.9725, -5.197, -5.2023, -5.4416, -5.5617, -5.6193, -5.5911, -5.6844, -5.7062, -5.7398, -5.7393, -5.7197, -5.8215, -5.8159, -5.8907, -5.9195, -5.971, -5.9543, -5.9916, -5.9281, -6.0816, -6.0841, -6.1807, -6.0962, -6.2182, -6.3234, -6.3024, -5.8423, -5.1315, -5.2061, -4.243, -4.9746, -4.8865, -4.393, -5.1886, -3.1635, -5.0844, -5.177, -5.1887, -5.2365, -5.2654, -5.2229, -5.0814, -5.5711, -5.6106, -5.5707, -5.3442, -5.6422, -5.6146, -4.2672, -4.634, -4.7927, -5.1269, -5.1256, -5.1315, -5.0768, -5.2037, -5.3932, -5.5251, -5.5368, -5.5551, -5.5221, -5.6098, -5.654, -5.7916, -5.8265, -5.7977, -5.6993, -5.8747, -5.9145, -5.9013, -5.9281, -6.0014, -6.0116, -6.0574, -6.1202, -6.2158, -5.926, -6.1623, -4.8813, -5.8158, -5.0106, -4.9297, -4.1262, -4.892, -2.8913, -4.6729, -5.1312, -5.5832, -5.469, -5.4196, -5.5382, -5.4913, -5.5049, -5.662, -4.6443, -3.9345, -4.5774, -5.1116, -5.4336, -5.3881, -5.4014, -5.5636, -5.6376, -5.6694, -5.6174, -5.5978, -5.739, -5.7203, -5.7566, -5.602, -5.922, -5.9375, -5.9623, -5.9483, -6.0382, -6.067, -6.1473, -6.1071, -6.1361, -6.0532, -6.2112, -6.2206, -6.2377, -6.2543, -4.6667, -5.2082, -5.8422, -4.2219, -5.2041, -4.2812, -5.0591, -2.7204, -5.3256, -5.3263, -5.4707, -5.6924, -5.4476, -5.57, -5.4214, -5.5551, -5.6694, -4.5842, -4.9249, -5.0852, -5.1608, -5.2318, -5.3013, -5.4761, -5.4878, -5.5081, -5.5362, -5.5655, -5.6376, -5.6658, -5.6616, -5.7253, -5.759, -5.8299, -5.8734, -5.8535, -6.0444, -5.9981, -6.0734, -6.3751, -6.3532, -6.352, -6.4385, -6.3792, -6.3957, -6.4128, -6.3868, -5.1542, -5.9957, -5.1991, -5.5749, -4.8167, -4.8354, -4.7776, -5.6776, -3.266, -4.9713, -5.3604, -4.7752, -4.8122, -5.1567, -5.3962, -5.5375, -5.5322, -5.6439, -5.6629], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.1506, 1.1503, 1.1503, 1.1503, 1.1501, 1.1501, 1.1497, 1.1493, 1.1493, 1.1492, 1.1492, 1.1491, 1.1489, 1.1489, 1.1488, 1.1487, 1.1486, 1.1484, 1.1483, 1.1482, 1.1482, 1.148, 1.1478, 1.1477, 1.1477, 1.1476, 1.1475, 1.1473, 1.1467, 1.1467, 1.1434, 1.1284, 1.0405, 0.8698, 0.954, 0.9251, 0.7234, 0.805, -0.1833, 0.7023, 0.6998, 0.6358, 0.6441, 0.5699, 0.414, 0.0875, 0.7218, 0.8043, 0.6501, -0.2655, 0.6846, -0.3024, 1.3174, 1.3173, 1.3171, 1.3167, 1.3166, 1.3166, 1.3166, 1.3165, 1.3162, 1.3159, 1.3158, 1.3158, 1.3157, 1.3155, 1.3153, 1.3152, 1.3151, 1.315, 1.3147, 1.3146, 1.3146, 1.3145, 1.3145, 1.3145, 1.3142, 1.3139, 1.3137, 1.3133, 1.3133, 1.3133, 1.3111, 1.3131, 1.293, 1.2197, 0.9525, 1.0665, 0.0889, 0.4959, 0.6597, 0.9386, 0.7845, 0.4049, 0.6097, 0.2326, 0.132, 0.495, 1.4201, 1.4201, 1.42, 1.4192, 1.4191, 1.419, 1.419, 1.4186, 1.4184, 1.4182, 1.4181, 1.418, 1.418, 1.4179, 1.4178, 1.4177, 1.4175, 1.4174, 1.4172, 1.4171, 1.4168, 1.4164, 1.4164, 1.4164, 1.4163, 1.4163, 1.416, 1.4159, 1.4157, 1.4156, 1.4105, 1.4042, 1.4117, 1.3715, 1.3401, 1.031, 1.1359, 0.2597, 1.1634, 1.0495, 1.0749, 1.2217, 0.7003, 0.82, 0.3025, 0.505, 0.8957, 1.7419, 1.7412, 1.7408, 1.7407, 1.7405, 1.7403, 1.7397, 1.7396, 1.7395, 1.7394, 1.7394, 1.7391, 1.739, 1.7388, 1.7386, 1.7384, 1.7382, 1.738, 1.7379, 1.7367, 1.7366, 1.7364, 1.7343, 1.7342, 1.734, 1.7339, 1.7338, 1.7336, 1.7334, 1.7332, 1.7151, 1.7276, 1.6297, 1.6004, 1.3637, 1.2247, 1.0133, 1.3579, -0.2858, 0.8135, 1.0928, 0.3377, 0.3042, 0.0121, 0.4845, 0.6535, 0.3032, 0.5128, 0.9722]}, \"token.table\": {\"Topic\": [2, 3, 4, 1, 2, 3, 4, 2, 1, 2, 3, 4, 3, 2, 3, 1, 2, 4, 2, 2, 2, 2, 2, 4, 4, 3, 2, 3, 1, 1, 3, 4, 2, 3, 4, 2, 2, 1, 4, 2, 3, 1, 3, 3, 4, 1, 2, 1, 4, 1, 3, 3, 1, 2, 3, 4, 2, 3, 4, 2, 1, 1, 2, 3, 1, 1, 2, 1, 2, 3, 4, 2, 4, 4, 1, 2, 4, 2, 2, 1, 1, 1, 2, 4, 1, 3, 4, 4, 2, 2, 1, 4, 1, 2, 4, 1, 3, 4, 4, 1, 4, 1, 2, 3, 1, 2, 1, 4, 4, 4, 3, 1, 3, 1, 3, 3, 4, 1, 4, 4, 1, 2, 4, 2, 2, 4, 2, 1, 2, 3, 1, 2, 4, 4, 1, 2, 3, 4, 1, 2, 4, 2, 3, 1, 1, 3, 3, 1, 2, 1, 2, 2, 2, 3, 1, 3, 1, 1, 2, 3, 4, 1, 2, 4, 1, 3, 4, 3, 1, 2, 3, 4, 1, 1, 2, 3, 4, 2, 3, 1, 3, 1, 3, 4, 2, 1, 4, 3, 3, 1, 3, 4, 3, 3, 1, 3, 4, 3, 1, 1, 2, 2, 3, 4, 1, 2, 3, 4, 4, 1, 3, 4, 4, 1, 1, 3, 2, 1, 1, 2, 3, 1, 2, 4, 3, 1, 1, 2, 3, 4, 3, 4, 1, 4, 1, 1, 2, 3, 3, 3, 3, 3, 2, 4, 3, 1, 2, 3, 4, 1, 2, 3, 1, 2, 3, 4, 1, 1, 2, 4, 2, 3, 4, 4, 3, 4, 1, 4, 2, 4, 2, 3, 2, 1, 2, 4, 1, 2, 4, 2], \"Freq\": [0.9952726714674903, 0.004739393673654716, 0.991712548238775, 0.6511933375463508, 0.16433901262999387, 0.09860340757799632, 0.08627798163074679, 0.9979789688018323, 0.242179550762338, 0.6935973531908773, 0.043921732128182206, 0.020130793892083512, 0.9919843679493181, 0.005369881036210359, 0.9880581106627061, 0.4211102883850151, 0.24301959373760781, 0.3357751638664658, 0.9965244106687925, 0.9942064718679842, 0.996573827409221, 0.9978934683987687, 0.9971331938324416, 0.9934569750694873, 0.9933838098450828, 0.9966118449122952, 0.9989270892784967, 0.9957809640802899, 0.9972063215188477, 0.007233884500245077, 0.47020249251593, 0.5208396840176456, 0.0008037669550379019, 0.9982785581570741, 0.9933812703854871, 0.9959143509892334, 0.9974458686793819, 0.1290724704911329, 0.8687570129210869, 0.9758426040078918, 0.02283886945550385, 0.9938498316482882, 0.008213634972299903, 0.9981956015208513, 0.9948311496507376, 0.9921970804097271, 0.9966568419066516, 0.7542678754332685, 0.24490036457582273, 0.0015286574423872723, 0.9982133098788888, 0.9986216847502243, 0.26316418704443845, 0.29248776900049867, 0.3129095492913263, 0.1314324834101985, 0.49221670340123025, 0.4868858365773902, 0.021323467295360155, 0.995927946028803, 0.9964346431545293, 0.2220343364932478, 0.7778553907611132, 0.9971812105770012, 0.9963630696829273, 0.7972228414980462, 0.20184463662132063, 0.8204959410302709, 0.10274036131161653, 0.02711203979056547, 0.04994323119314693, 0.9969189177998461, 0.9854589639085398, 0.9962188194819959, 0.6058763309191457, 0.3153614971661169, 0.07836255384127752, 0.9985327146472187, 0.9993525697905503, 0.9975693980324267, 0.9984452702553429, 0.27196522015318075, 0.043169082563996944, 0.6820715045111517, 0.40722835932402257, 0.5906159648474235, 0.9977387307127944, 0.9957313232742665, 0.9953170278442439, 0.9970143609225848, 0.9940663564029697, 0.9926081818024328, 0.6518144184955716, 0.11085279226115163, 0.2369082531752612, 0.8943308450564701, 0.06079880744901441, 0.045108792623462304, 0.9965280004446087, 0.9988716086349009, 0.9936149630248495, 0.9984327428180021, 0.0099331534088041, 0.988348764176008, 0.9981982270895362, 0.9921135043886298, 0.31575537579725044, 0.6847485765835721, 0.9976456533924998, 0.9969054521801863, 0.9959654087442646, 0.6268985337626689, 0.371888960706668, 0.9933117311447615, 0.9907136873880898, 0.9942688698493086, 0.993792573111338, 0.9985328285692388, 0.9946521853159458, 0.9988930148861918, 0.602574202762648, 0.112897649727539, 0.2842844432898271, 0.9977744300120212, 0.9982543541189922, 0.990141695561388, 0.9983385763946314, 0.596659326154184, 0.40120196068988234, 0.0012859037201598793, 0.41075333679272424, 0.5865083703242264, 0.003949551315314656, 0.9958208697523095, 0.343577264063209, 0.2570650033278686, 0.0037076683172288737, 0.3942487310653369, 0.4052503288510748, 0.4393200467633333, 0.15600344517718367, 0.9982907771653549, 0.9987769230926162, 0.9960746819076175, 0.9972140318001949, 0.9915033400335914, 0.9943525509422108, 0.7065860915594289, 0.2923804516797637, 0.9982769550352218, 0.9983465545505422, 0.9980490885269898, 0.04796890334182077, 0.9512131471186588, 0.2274173913968089, 0.7722194938638896, 0.9949613051928301, 0.029093108903181675, 0.2644828082107425, 0.7061690979226825, 0.9982281832918038, 0.6368117249580147, 0.2574345271106868, 0.10568364797175564, 0.9969692155342862, 0.008587681815418139, 0.987583408773086, 0.9930020901345324, 0.32909151142814386, 0.33723158414898136, 0.32676577636504744, 0.006977205189289269, 0.9989283629046936, 0.34511508246721573, 0.43923737768554727, 0.038716972501157666, 0.17689651228977207, 0.9989569113064068, 0.9927087222989073, 0.9778886603702628, 0.02186336435787173, 0.24400100258508603, 0.7506290384869441, 0.003725206146337191, 0.9968064971532857, 0.9993524378288121, 0.989004293377875, 0.9916218043295196, 0.9985683505051829, 0.09478954452025126, 0.01404289548448167, 0.891723863264586, 0.9981963546984558, 0.9955906134623254, 0.076589526346403, 0.9217153343067119, 0.9874012307529579, 0.9931261570258473, 0.9978263078168857, 0.005573426767394817, 0.9920699645962775, 0.9950125326924871, 0.9980300225718076, 0.996711137228518, 0.47862244708975776, 0.30486864113066975, 0.10233352989001501, 0.11405924685657924, 0.9980167939104028, 0.004882941306197578, 0.4004011871082014, 0.5957188393561045, 0.992534839958745, 0.9982419445544773, 0.9986492095613575, 0.9991506967384173, 0.9971915205965505, 0.9950370135573826, 0.9993182698480947, 0.0169661081650585, 0.984034273573393, 0.5589990959474068, 0.2040996699156811, 0.23659961735448382, 0.9946633231901575, 0.9985896321277756, 0.2334397433555395, 0.002311284587678609, 0.6764359559939396, 0.08705838613589427, 0.02558805441070717, 0.9723460676068724, 0.9955925407504981, 0.9979710282014176, 0.999121309155334, 0.6377092816162743, 0.26003679444547106, 0.10153817687870775, 0.9962855858038686, 0.9945476943544691, 0.9931404711880649, 0.9969701082974454, 0.9995877038851944, 0.9959671120025655, 0.9959665096811391, 0.1290250067972191, 0.3123763322458989, 0.5477903797355618, 0.009054386441910112, 0.03794072702610997, 0.2722805115991421, 0.6896285088863517, 0.4302530457633416, 0.24381005926589358, 0.03406169945626455, 0.2922135269142695, 0.9981170766598614, 0.998069948417843, 0.5351033152425505, 0.4627920564259896, 0.0038231898307143504, 0.818162623772871, 0.17968992204357445, 0.9929916875554234, 0.9949085599237159, 0.9887567087539344, 0.996728057273804, 0.9924325847933586, 0.5172717294403051, 0.4812119213783608, 0.6843579179403761, 0.3150628905234939, 0.9985363478982948, 0.017796925473475052, 0.9058635065998802, 0.07474708698859522, 0.7055061515064031, 0.1578105865211691, 0.13460314732687953, 0.9978922327575274], \"Term\": [\"affect\", \"affect\", \"age\", \"also\", \"also\", \"also\", \"also\", \"americans\", \"amp\", \"amp\", \"amp\", \"amp\", \"announce\", \"august\", \"august\", \"back\", \"back\", \"back\", \"become\", \"behindnepconbykiit\", \"believe\", \"best\", \"biden\", \"blame\", \"book\", \"break\", \"bring\", \"business\", \"call\", \"care\", \"care\", \"care\", \"case\", \"case\", \"check\", \"child\", \"children\", \"close\", \"close\", \"come\", \"come\", \"conduct\", \"conduct\", \"confirm\", \"consider\", \"control\", \"corner\", \"corona\", \"corona\", \"coronavirus\", \"coronavirus\", \"countries\", \"covid\", \"covid\", \"covid\", \"covid\", \"deaths\", \"deaths\", \"deaths\", \"decision\", \"didnt\", \"die\", \"die\", \"distance\", \"doesnt\", \"dont\", \"dont\", \"due\", \"due\", \"due\", \"due\", \"education\", \"especially\", \"europe\", \"even\", \"even\", \"even\", \"every\", \"everyone\", \"exam\", \"exams\", \"fight\", \"fight\", \"fight\", \"find\", \"find\", \"first\", \"flight\", \"flu\", \"follow\", \"free\", \"fuck\", \"get\", \"get\", \"get\", \"give\", \"give\", \"give\", \"good\", \"govt\", \"great\", \"happen\", \"health\", \"health\", \"help\", \"high\", \"home\", \"home\", \"hope\", \"house\", \"increase\", \"india\", \"india\", \"infect\", \"information\", \"issue\", \"ive\", \"jee\", \"kid\", \"kill\", \"know\", \"know\", \"know\", \"learn\", \"leave\", \"level\", \"lie\", \"like\", \"like\", \"like\", \"live\", \"live\", \"live\", \"love\", \"make\", \"make\", \"make\", \"make\", \"many\", \"many\", \"many\", \"market\", \"mask\", \"may\", \"mean\", \"measure\", \"million\", \"need\", \"need\", \"neet\", \"nep\", \"never\", \"new\", \"new\", \"news\", \"news\", \"next\", \"number\", \"number\", \"number\", \"nurse\", \"one\", \"one\", \"one\", \"open\", \"others\", \"others\", \"outbreak\", \"pandemic\", \"pandemic\", \"pandemic\", \"pandemic\", \"pay\", \"people\", \"people\", \"people\", \"people\", \"plan\", \"play\", \"please\", \"please\", \"positive\", \"positive\", \"positive\", \"post\", \"postpone\", \"prove\", \"provide\", \"read\", \"really\", \"really\", \"really\", \"record\", \"recover\", \"report\", \"report\", \"require\", \"research\", \"return\", \"right\", \"right\", \"rise\", \"risk\", \"safe\", \"say\", \"say\", \"say\", \"say\", \"school\", \"see\", \"see\", \"see\", \"set\", \"sir\", \"situation\", \"social\", \"still\", \"student\", \"students\", \"support\", \"support\", \"take\", \"take\", \"take\", \"talk\", \"tell\", \"test\", \"test\", \"test\", \"test\", \"thank\", \"thank\", \"thing\", \"things\", \"think\", \"time\", \"time\", \"time\", \"today\", \"total\", \"train\", \"travel\", \"trump\", \"tweet\", \"update\", \"vaccine\", \"vaccine\", \"vaccine\", \"vaccine\", \"via\", \"via\", \"via\", \"virus\", \"virus\", \"virus\", \"virus\", \"want\", \"watch\", \"way\", \"way\", \"wear\", \"wear\", \"wear\", \"white\", \"without\", \"wonder\", \"wont\", \"word\", \"work\", \"work\", \"world\", \"world\", \"worst\", \"would\", \"would\", \"would\", \"year\", \"year\", \"year\", \"years\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 2, 4, 1]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el1081401495244848884715974756\", ldavis_el1081401495244848884715974756_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el1081401495244848884715974756\", ldavis_el1081401495244848884715974756_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el1081401495244848884715974756\", ldavis_el1081401495244848884715974756_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ],
            "text/plain": [
              "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
              "topic                                                \n",
              "2     -0.114731 -0.064136       1        1  31.614771\n",
              "1     -0.040581 -0.187376       2        1  26.758898\n",
              "3      0.272137  0.037075       3        1  24.134130\n",
              "0     -0.116826  0.214437       4        1  17.492201, topic_info=              Term         Freq        Total Category  logprob  loglift\n",
              "3904          case  1244.000000  1244.000000  Default  30.0000  30.0000\n",
              "27880        trump   988.000000   988.000000  Default  29.0000  29.0000\n",
              "18369          new   979.000000   979.000000  Default  28.0000  28.0000\n",
              "5497        corona  1584.000000  1584.000000  Default  27.0000  27.0000\n",
              "5589   coronavirus   654.000000   654.000000  Default  26.0000  26.0000\n",
              "...            ...          ...          ...      ...      ...      ...\n",
              "15033         know   208.761878   735.179166   Topic4  -5.3962   0.4845\n",
              "1997          back   181.246611   539.051185   Topic4  -5.5375   0.6535\n",
              "26510         take   182.209615   769.232013   Topic4  -5.5322   0.3032\n",
              "29176        virus   162.948367   557.811275   Topic4  -5.6439   0.5128\n",
              "29545          way   159.882754   345.727628   Topic4  -5.6629   0.9722\n",
              "\n",
              "[224 rows x 6 columns], token_table=       Topic      Freq    Term\n",
              "term                          \n",
              "452        2  0.995273  affect\n",
              "452        3  0.004739  affect\n",
              "519        4  0.991713     age\n",
              "851        1  0.651193    also\n",
              "851        2  0.164339    also\n",
              "...      ...       ...     ...\n",
              "30276      4  0.074747   would\n",
              "30453      1  0.705506    year\n",
              "30453      2  0.157811    year\n",
              "30453      4  0.134603    year\n",
              "30479      2  0.997892   years\n",
              "\n",
              "[280 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[3, 2, 4, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPyCX7-2NB_W",
        "colab_type": "text"
      },
      "source": [
        "## **D. Text Network Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXKbtyB8KIZP",
        "colab_type": "text"
      },
      "source": [
        "Though network analysis is most often used to describe relationships between people, some of the early pioneers of network analysis realized that it could also be applied to represent relationships between words. For example, one can represent a corpus of documents as a network where each node is a document, and the thickness or strength of the edges between them describes similarities between the words used in any two documents. Or, one can create a textnetwork where individual words are the nodes, and the edges between them describe the regularity with which they co-occur in documents.\n",
        "\n",
        "There are multiple advantages to a network-based approach to automated text analysis. Just as clusters of social connections can help explain a range of outcomes, understanding patterns of connections between words helps identify their meaning in a more precise manner.Second, text networks can be built out of documents of any length, whereas topic models function poorly on short texts such as social media messages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQhsmBwSKxl3",
        "colab_type": "text"
      },
      "source": [
        "In this prcatice we will use NetworkX. NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks. You can see the full documentation of NetworkX HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vroZ-J0c2QXR",
        "colab_type": "text"
      },
      "source": [
        "Here we construct a text network based on conversations about 'Demonetization in India'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajIRyYeAW9lq",
        "colab_type": "text"
      },
      "source": [
        "**Install & Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iv2l0EPlXXLK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import nltk\n",
        "import itertools\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from nltk import bigrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "from random import seed\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb8Q4cX8IqIv",
        "colab_type": "text"
      },
      "source": [
        "**Import Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9Saf4RFIout",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Data\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/sma-health/data/master/text_preprocessed_covid19_short.csv', sep = ',')\n",
        "\n",
        "# Show Data\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrOmikMd1hky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert to String\n",
        "df['text'] = df['text'].fillna('').apply(str)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4Sl8MO_y5h-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select Text\n",
        "text = df['text']\n",
        "text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBY920BBAhKW",
        "colab_type": "text"
      },
      "source": [
        "### **Preparing Adjacency Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQslFdEkzOpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize\n",
        "text_data = [word_tokenize(i) for i in text]\n",
        "print(text_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAMLUtpR13Kb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create Fuction to show co occurrence\n",
        "def generate_co_occurrence_matrix(corpus):\n",
        "    vocab = set(corpus)\n",
        "    vocab = list(vocab)\n",
        "    vocab_index = {word: i for i, word in enumerate(vocab)}\n",
        " \n",
        "    # Create bigrams from all words in corpus\n",
        "    bi_grams = list(bigrams(corpus))\n",
        " \n",
        "    # Frequency distribution of bigrams ((word1, word2), num_occurrences)\n",
        "    bigram_freq = nltk.FreqDist(bi_grams).most_common(len(bi_grams))\n",
        " \n",
        "    # Initialise co-occurrence matrix\n",
        "    # co_occurrence_matrix[current][previous]\n",
        "    co_occurrence_matrix = np.zeros((len(vocab), len(vocab)))\n",
        " \n",
        "    # Loop through the bigrams taking the current and previous word,\n",
        "    # and the number of occurrences of the bigram.\n",
        "    for bigram in bigram_freq:\n",
        "        current = bigram[0][1]\n",
        "        previous = bigram[0][0]\n",
        "        count = bigram[1]\n",
        "        pos_current = vocab_index[current]\n",
        "        pos_previous = vocab_index[previous]\n",
        "        co_occurrence_matrix[pos_current][pos_previous] = count\n",
        "    co_occurrence_matrix = np.matrix(co_occurrence_matrix)\n",
        " \n",
        "    # return the matrix and the index\n",
        "    return co_occurrence_matrix, vocab_index"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT7Vqips15iS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create one list using many lists\n",
        "data = list(itertools.chain.from_iterable(text_data))\n",
        "matrix, vocab_index = generate_co_occurrence_matrix(data)\n",
        " \n",
        " \n",
        "data_matrix = pd.DataFrame(matrix, index=vocab_index,\n",
        "                             columns=vocab_index)\n",
        "\n",
        "# Show Adjacency Matrix\n",
        "data_matrix.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IHqMGi96bvr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_matrix.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9MLNkEJBFZx",
        "colab_type": "text"
      },
      "source": [
        "### **Constructing Text Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bq8InbQzGf5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Construct Network form Data Matrix\n",
        "G = nx.from_pandas_adjacency(data_matrix)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9XH2HTw07CZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make A Random Sample of the node\n",
        "import random\n",
        "k = 500\n",
        "\n",
        "sampled_nodes = random.sample(G.nodes, k)\n",
        "sampled_graph = G.subgraph(sampled_nodes)\n",
        "\n",
        "# Visualize the Network\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10,10))\n",
        "nx.draw(sampled_graph, with_labels=True, \n",
        "        node_color='skyblue', node_size=600, \n",
        "        arrowstyle='->',arrowsize=20, edge_color='r',\n",
        "        font_size=7,\n",
        "        pos=nx.kamada_kawai_layout(sampled_graph))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XibgWnJBoNL",
        "colab_type": "text"
      },
      "source": [
        "### **Network Metrics and Measurement**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47fv2rgnKShO",
        "colab_type": "text"
      },
      "source": [
        "**Centrality Measurement**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eonE6ffXKfqU",
        "colab_type": "text"
      },
      "source": [
        "In graph theory and network analysis, indicators of centrality identify the most important vertices within a graph. Applications include identifying the most influential person(s) in a social network, key infrastructure nodes in the Internet or urban networks, and super-spreaders of disease. Centrality concepts were first developed in social network analysis, and many of the terms used to measure centrality reflect their sociological origin."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qG3tJEkqKVPc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Degree Centrality\n",
        "degree = nx.degree_centrality(sampled_graph)\n",
        "\n",
        "# Sorted from the Highest\n",
        "sorted(nx.degree(sampled_graph), key=lambda x: x[1], reverse=True)[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88vr0guJOVuz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Betweenness Centrality\n",
        "betweenness = nx.betweenness_centrality(sampled_graph)\n",
        "\n",
        "# Sorted from the Highest\n",
        "sorted(nx.betweenness_centrality(sampled_graph, normalized=True).items(), key=lambda x:x[1], reverse=True)[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRKPlKYsPGMx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Closeness Centrality\n",
        "closeness = nx.closeness_centrality(sampled_graph)\n",
        "\n",
        "# Sorted from the Highest\n",
        "sorted(nx.closeness_centrality(sampled_graph).items(), key=lambda x:x[1], reverse=True)[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XWM0uLSPdSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Eigenvector Centrality\n",
        "eigenvector = nx.eigenvector_centrality(sampled_graph)\n",
        "\n",
        "# Sorted from the Highest\n",
        "sorted(nx.eigenvector_centrality(sampled_graph).items(), key=lambda x:x[1], reverse=True)[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMxWj0m7De5J",
        "colab_type": "text"
      },
      "source": [
        "***Visualize Network based on Centrality Measurement***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whzLeVNKbPj1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set Degree Dictionary\n",
        "d = dict(degree)\n",
        "\n",
        "# Visualize the Network\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10,10))\n",
        "nx.draw(sampled_graph, with_labels=True, \n",
        "        node_color='skyblue', nodelist=d.keys(),\n",
        "        node_size=[v * 50000 for v in d.values()], \n",
        "        arrowstyle='->',arrowsize=20, edge_color='r',\n",
        "        font_size=8,\n",
        "        pos=nx.kamada_kawai_layout(sampled_graph))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9j8PrQWwUA7",
        "colab_type": "text"
      },
      "source": [
        "**Network Topology Measurement**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLNTtHEkRWK-",
        "colab_type": "text"
      },
      "source": [
        "The configuration, or topology, of a network is key to determining its performance. Network topology is the way a network is arranged, including the physical or logical description of how links and nodes are set up to relate to each other."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLuKZD791wb5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Show Number of Nodes\n",
        "nx.number_of_nodes(sampled_graph)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHT180js1lHr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Show Number of Edges\n",
        "nx.number_of_edges(sampled_graph)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6W83y_4xpVK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Show Graph Density\n",
        "nx.density(sampled_graph)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52AuWdV1BtUZ",
        "colab_type": "text"
      },
      "source": [
        "### **Community Detection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qIbaO55RvCz",
        "colab_type": "text"
      },
      "source": [
        "Community detection is a fundamental problem in dividing text (modelled as nodes in a social graph) with certain word connections into densely knitted and highly related groups with each group well separated from different group members."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPq8nalnLduK",
        "colab_type": "text"
      },
      "source": [
        "**Modularity Community**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCv3Y88JeH1d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Module\n",
        "from networkx.algorithms.community import greedy_modularity_communities\n",
        "\n",
        "# Modularity Community Detection\n",
        "communities_m = sorted(greedy_modularity_communities(sampled_graph), key=len, reverse=True)\n",
        "communities_m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dwxt7KhbHK-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set Node Community Function\n",
        "def set_node_community(sampled_graph, communities_m):\n",
        "      '''Add community to node attributes'''\n",
        "      for c, v_c in enumerate(communities_m):\n",
        "        for v in v_c:\n",
        "          # Add 1 to save 0 for external edges\n",
        "          sampled_graph.nodes[v]['community'] = c + 1      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujM90pkvH3ME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set Colour Function\n",
        "def get_color(i, r_off=1, g_off=1, b_off=1):\n",
        "     '''Assign a color to a vertex.'''\n",
        "     r0, g0, b0 = 0, 0, 0\n",
        "     n = 16\n",
        "     low, high = 0.1, 0.9\n",
        "     span = high - low\n",
        "     r = low + span * (((i + r_off) * 3) % n) / (n - 1)\n",
        "     g = low + span * (((i + g_off) * 5) % n) / (n - 1)\n",
        "     b = low + span * (((i + b_off) * 7) % n) / (n - 1)\n",
        "     return (r, g, b) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9uXID-6IJfJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set Node Communities\n",
        "community = set_node_community(sampled_graph, communities_m)\n",
        "\n",
        "# Set Node Color\n",
        "node_color = [get_color(sampled_graph.nodes[v]['community']) for v in sampled_graph.nodes]\n",
        "\n",
        "# Visualize the Network\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10,10))\n",
        "nx.draw(sampled_graph, with_labels=True, \n",
        "        node_color = node_color, node_size=600, \n",
        "        arrowstyle='->',arrowsize=20, edge_color='r',\n",
        "        font_size=7, map = plt.get_cmap('jet'),\n",
        "        pos=nx.kamada_kawai_layout(sampled_graph))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87CZow4LNG0c",
        "colab_type": "text"
      },
      "source": [
        "## **E. Word Cloud**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwoMONXRViNu",
        "colab_type": "text"
      },
      "source": [
        "A word cloud is a collection, or cluster, of words depicted in different sizes. The bigger and bolder the word appears, the more often it’s mentioned within a given text and the more important it is.\n",
        "\n",
        "Also known as tag clouds or text clouds, these are ideal ways to pull out the most pertinent parts of textual data, from blog posts to databases. They can also help business users compare and contrast two different pieces of text to find the wording similarities between the two."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNOv2JnJNK4W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Libraries\n",
        "import wordcloud\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5t19w8uySBXS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Modules\n",
        "from wordcloud import WordCloud\n",
        "from wordcloud import STOPWORDS\n",
        "\n",
        "# Build Word Cloud\n",
        "text = \" \".join(review for review in df.text)\n",
        "cloud = WordCloud(background_color='white').generate(text)\n",
        "plt.figure(figsize=(7, 7), facecolor=None)\n",
        "plt.imshow(cloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdRf95s9QUmZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save File\n",
        "cloud.to_file(\"wordcloud.png\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}